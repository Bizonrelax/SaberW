## Текстовый план программы "Saber W"

### 1. Общая концепция
**Saber W** — программа для лексического сжатия текста путём замены повторяющихся слов и фраз на короткие коды. Цель — экономия токенов в контекстном окне нейросетей.

---

### 2. Основные правила

#### 2.1 Что кодируем
- **Слова**: любые последовательности букв и цифр длиной ≥4 символа, встречающиеся ≥2 раза.
- **Фразы**: любые последовательности слов (включая пробелы, пунктуацию, табуляции, переносы строк) общей длиной ≥7 символов, встречающиеся ≥2 раза.
- **Частота**: минимальная частота для кодирования — 2 повторения.

#### 2.2 Формат кодирования
- **Маркер кода**: символ `◌` (U+25CC, DOTTED CIRCLE) или другой редкий символ.
- **Одиночное слово/фраза**: `◌X`, где X — символ-код.
- **Последовательность кодов**: `◌NXXX...`, где N — количество кодов, XXX... — сами коды.
- **Пример**:  
  `◌A` — одно закодированное слово.  
  `◌3ABC` — три закодированных слова подряд с кодами A, B, C.

#### 2.3 Словарь
- Формат: текстовый, построчный `код=значение`.
- Пример:
  ```
  # SABER_W DICTIONARY v1 #
  A=программа
  B=сжатие
  C=текста
  # END #
  ```

---

### 3. Алгоритм работы

#### Этап 1: Подготовка текста
1. Сохранить оригинальный текст с сохранением всех пробелов, переносов, табуляций.
2. Выделить *токены*:
   - Слова: последовательности `[A-Za-zА-Яа-я0-9]`.
   - Разделители: всё остальное (пробелы, знаки пунктуации и т.д.).

#### Этап 2: Поиск кандидатов для кодирования
1. **Слова**:
   - Найти все слова длиной ≥4 символа.
   - Посчитать частоту каждого слова.
   - Отфильтровать: частота ≥2.

2. **Фразы**:
   - Найти все последовательности токенов длиной от 2 до *M* токенов (*M* — настройка, например, 5).
   - Для каждой последовательности:
     - Объединить токены в строку (сохраняя оригинальные разделители).
     - Посчитать длину строки (символов).
     - Посчитать частоту повторения идентичной строки.
   - Отфильтровать: длина ≥7 символов, частота ≥2.

3. **Объединение кандидатов**:
   - Создать общий список кандидатов (слов и фраз).
   - Для каждого кандидата вычислить **приоритет**:
     ```
     приоритет = (длина_кандидата - 1) * (частота - 1)
     ```
     (длина кода принята за 1, первое вхождение не кодируется)

#### Этап 3: Построение словаря
1. Отсортировать кандидатов по убыванию приоритета.
2. Назначить коды:
   - Использовать символы из CJK Extension A (U+3400–U+4DBF), так как они, вероятно, занимают 1 токен и не встречаются в обычных текстах.
   - Начинать с первого символа (U+3400) и двигаться по порядку.
   - Если кандидатов больше, чем доступных символов, перейти к двухсимвольным кодам (например, U+3400+U+3400, U+3400+U+3401…).

3. **Оптимизация словаря**:
   - Если фраза кодируется, то слова внутри неё кодируются только если они сами по себе встречаются ≥2 раз *вне* этой фразы.
   - Удалить из словаря коды, которые не были использованы при кодировании (например, если слово встречалось только внутри фраз, которые закодированы).

#### Этап 4: Кодирование текста
1. Заменить в тексте все вхождения кандидатов (согласно словарю) на коды с маркером.
   - Заменять **все** вхождения, а не только со второго (так как частота ≥2 гарантирует выгоду).
   - Заменять сначала самые длинные фразы, затем более короткие, чтобы избежать конфликтов.
2. Применить **групповое кодирование**:
   - Если в тексте подряд идут несколько закодированных элементов, заменить их на последовательность с указанием количества.
   - Пример: `◌A◌B◌C` → `◌3ABC`.

#### Этап 5: Экспорт
1. Сформировать словарь в текстовом формате.
2. Выдать пользователю:
   - Словарь.
   - Сжатый текст.

---

### 4. Технические детали

#### 4.1. Обработка вложенности и перекрытий
- Фразы могут перекрываться. Алгоритм замены:
  1. Отсортировать фразы по убыванию длины.
  2. Итерироваться по тексту, находить вхождения фраз.
  3. При нахождении вхождения заменять его на код, пропуская уже заменённые участки.

#### 4.2. Экранирование
- Проверить, содержит ли исходный текст символы, используемые как маркеры или коды.
- Если содержит — предупредить пользователя и предложить:
  - Автоматически экранировать (удвоить символ).
  - Выбрать другие маркеры/коды.

#### 4.3. Токенизация для нейросетей
- Учитывать, что пробелы и некоторые знаки пунктуации могут считаться отдельными токенами.
- Поэтому при расчёте экономии учитывать, что маркер `◌` и символ кода — это 2 токена (если нейросеть разбивает так), а не 1.

---

### 5. Этапы реализации (кратко)

1. **Прототип на Java**:
   - Класс `SaberW` с методами `compress(String text)` и `decompress(String compressed, String dictionary)`.
   - Классы `Token`, `Candidate`, `Dictionary`.

2. **Тестирование**:
   - На коротких текстах, затем на длинных (исходный код, документация).
   - Проверка корректности восстановления.

3. **Оптимизация**:
   - Алгоритм поиска фраз (суффиксные деревья для больших текстов).
   - Учёт токенизации нейросетей (экспериментальное определение «веса» символов).

4. **Доработка**:
   - Настройки через GUI (минимальная длина слова/фразы, максимальная длина фразы в токенах).
   - Автоматический выбор маркера.

---

### 6. Вопросы для дальнейшего обсуждения

1. **Маркер кода**: точно ли использовать `◌` (U+25CC)? Он может плохо отображаться в некоторых шрифтах. Альтернатива — `‧` (U+2027) или `・` (U+30FB).
2. **Кодирование последовательностей**: стоит ли делать групповое кодирование обязательным? Может, оставить только одиночные коды для простоты.
3. **Экономия токенов**: как точно оценить, сколько токенов сэкономит кодирование? Нужно ли интегрировать с токенизатором (например, tiktoken для GPT)?
4. **Декодирование**: будем ли делать функцию декодирования в самой программе? Или только кодирование, а нейросеть будет декодировать «в уме»?

---

### 7. Заключение

План реализуем, но требует аккуратной проработки алгоритмов поиска фраз и замены. Начнём с простой версии (только слова), затем добавим фразы и групповое кодирование.

**Следующий шаг**: если согласен, приступаем к проектированию классов и написанию кода. Или обсуждаем конкретные вопросы.